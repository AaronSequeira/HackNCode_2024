{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jenil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt to /Users/jenil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /Users/jenil/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jenil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jenil/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt to /Users/jenil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing library\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "import scipy\n",
    "from string import punctuation\n",
    "from nltk import tokenize\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor\n",
    "import yake\n",
    "from keybert import KeyBERT\n",
    "from summarizer import Summarizer\n",
    "import nltk\n",
    "import io\n",
    "from IPython.display import Markdown, display\n",
    "from random import shuffle\n",
    "from io import StringIO\n",
    "nltk.download('stopwords')\n",
    "nltk.download('popular')\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import benepar\n",
    "nltk.download('punkt')\n",
    "benepar.download('benepar_en3')\n",
    "import benepar\n",
    "parser = benepar.Parser(\"benepar_en3\")\n",
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "# Importing custom python module\n",
    "import TrueAndFalse\n",
    "import Generatingblanks\n",
    "import GeneratingDistractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coordinate covalent bonding', 'shared electron pairs', 'dative bonding', 'covalent bonding', 'concept of shared', 'electron pair', 'electron', 'coordinate bonds', 'Lewis acid-base theory', 'Lewis', 'bonding', 'shared electron', 'Lewis acid-base', 'Lewis acid', 'Coordinate', 'electron density', 'bonds', 'atom', 'familiar covalent bond', 'Coordinate covalent', 'bonding', 'covalent', 'molecules', 'complexes', 'electron']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "full_text = open('5g.txt', 'r').read()\n",
    "mcq=1\n",
    "fib=0\n",
    "bool=0\n",
    "if mcq :\n",
    "    # Summarizing text\n",
    "    summarized_text = Generatingblanks.bert_summ(full_text)\n",
    "    # Keyword selection (Summarized Text)\n",
    "    kw_summ = Generatingblanks.keyword_selection(summarized_text)\n",
    "    # Keyword selection (full text)\n",
    "    kw_full = Generatingblanks.keyword_selection(full_text)\n",
    "    print(kw_summ)\n",
    "    # Lowering text from keywords by summarized text\n",
    "    filtered_keys1=[]\n",
    "    for keyword in kw_summ :\n",
    "        if keyword.lower() in summarized_text.lower():\n",
    "            filtered_keys1.append(keyword)\n",
    "        \n",
    "    # Lowering text from keywords by full text      \n",
    "    filtered_keys2 = []\n",
    "    for keyword in kw_full  :\n",
    "        if keyword.lower() in full_text.lower():\n",
    "            filtered_keys2.append(keyword)\n",
    "\n",
    "    # Mapping keyword with the sentence (summarized text)\n",
    "    sentences1 = Generatingblanks.SentenceMapping.tokenize_sentences(summarized_text)\n",
    "    keyword_sentence_mapping1 = Generatingblanks.SentenceMapping.get_sentences_for_keyword(filtered_keys1, sentences1)\n",
    "\n",
    "    # Mapping keyword with the sentence (full text)\n",
    "    sentences2 = Generatingblanks.SentenceMapping.tokenize_sentences(full_text)\n",
    "    keyword_sentence_mapping2 = Generatingblanks.SentenceMapping.get_sentences_for_keyword(filtered_keys2, sentences2)\n",
    "\n",
    "    # Combining both the keyword Dictionary\n",
    "    kws_m =  keyword_sentence_mapping2.copy()\n",
    "    kws_m.update(keyword_sentence_mapping1)\n",
    "    print(kws_m)\n",
    "    GeneratingDistractor.generate_options(kws_m)\n",
    "    print(\"done\")\n",
    "\n",
    "# Fill in the blanks\n",
    "if fib :\n",
    "    # Summarizing text\n",
    "    summarized_text = Generatingblanks.bert_summ(full_text)\n",
    "    # Keyword selection (Summarized Text)\n",
    "    kw_summ = Generatingblanks.keyword_selection(summarized_text)\n",
    "    # Keyword selection (full text)\n",
    "    kw_full = Generatingblanks.keyword_selection(full_text)\n",
    "\n",
    "    # Lowering text from keywords by summarized text\n",
    "    filtered_keys1=[]\n",
    "    for keyword in kw_summ :\n",
    "        if keyword.lower() in summarized_text.lower():\n",
    "            filtered_keys1.append(keyword)\n",
    "\n",
    "    # Lowering text from keywords by full text      \n",
    "    filtered_keys2 = []\n",
    "    for keyword in kw_full  :\n",
    "        if keyword.lower() in full_text.lower():\n",
    "            filtered_keys2.append(keyword)\n",
    "\n",
    "    # Mapping keyword with the sentence (summarized text)\n",
    "    sentences1 = Generatingblanks.SentenceMapping.tokenize_sentences(summarized_text)\n",
    "    keyword_sentence_mapping1 = Generatingblanks.SentenceMapping.get_sentences_for_keyword(filtered_keys1, sentences1)\n",
    "\n",
    "    # Mapping keyword with the sentence (full text)\n",
    "    sentences2 = Generatingblanks.SentenceMapping.tokenize_sentences(full_text)\n",
    "    keyword_sentence_mapping2 = Generatingblanks.SentenceMapping.get_sentences_for_keyword(filtered_keys2, sentences2)\n",
    "\n",
    "    # Combining both the keyword Dictionary\n",
    "    kws_m =  keyword_sentence_mapping2.copy()\n",
    "    kws_m.update(keyword_sentence_mapping1)\n",
    "\n",
    "    # Code for generating fill in the blanks as output\n",
    "    index = 1\n",
    "    for x,y in kws_m.items(): \n",
    "        sentence = y[0]\n",
    "        pattern = re.compile(x, re.IGNORECASE)\n",
    "        output = pattern.sub( \" _______ \", sentence)\n",
    "        print(\"%s)\"%(index),output)\n",
    "        index = index + 1 \n",
    "        print('Answer is',x)\n",
    "\n",
    "# True & False selected\n",
    "if bool :\n",
    "    # Summarizing the text\n",
    "    cand_sents = TrueAndFalse.BoolSumText.get_candidate_sents(full_text)\n",
    "    # Removing quoted sentences\n",
    "    filter_quotes_and_questions = TrueAndFalse.BoolSumText.preprocess(cand_sents)\n",
    "    sent_completion_dict = TrueAndFalse.get_sentence_completions(filter_quotes_and_questions)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    print(\"hello\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id)\n",
    "    model_BERT = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    print('***********')\n",
    "    index = 1\n",
    "    choice_list = [\"a)\",\"b)\",\"c)\",\"d)\",\"e)\",\"f)\"]\n",
    "    for key_sentence in sent_completion_dict:\n",
    "        partial_sentences = sent_completion_dict[key_sentence]\n",
    "        false_sentences =[]\n",
    "        print_string = \"**%s) True Sentence (from the story) :**\"%(str(index))\n",
    "        print(print_string)\n",
    "        print(\"  \",key_sentence)\n",
    "        for partial_sent in partial_sentences:\n",
    "            # Generating false statements\n",
    "            false_sents = TrueAndFalse.GenerateFalseText.generate_sentences(partial_sent,key_sentence)\n",
    "            false_sentences.extend(false_sents)\n",
    "        print(\"  **False Sentences (GPT-2 Generated)**\")\n",
    "        # Printing output for True and False sentences\n",
    "        for ind,false_sent in enumerate(false_sentences):\n",
    "            print_string_choices = \"**%s** %s\"%(choice_list[ind],false_sent)\n",
    "            #printmd(print_string_choices)\n",
    "            print_string_choices\n",
    "        index = index+1\n",
    "        print('Executing...')\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decimer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
